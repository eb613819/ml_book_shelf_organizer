# ğŸ“š Problem Description: Limitations of K-Means for Book Shelf Clustering

I'm organizing my book collection onto a fixed number of shelves by clustering books based on metadata like genre, author, series, and language. I initially chose **K-Means clustering** because it provides a straightforward mapping between the number of clusters and the number of shelves.

However, after testing and reviewing the results, I encountered several limitations with K-Means in this context:

## âš ï¸ Issues with K-Means

### 1. Assumes Spherical, Equally-Sized Clusters
- K-Means tends to form clusters of similar size and shape.
- This doesn't match the natural structure of my library, where some clusters (e.g., large series or popular genres) are much larger than others.

### 2. Insensitive to Categorical/One-Hot Features
- Most of my book metadata is **categorical** (e.g., genre tags, language, author, series).
- After one-hot encoding, K-Means doesnâ€™t handle these features well. All distances are treated equally, even if some features (like series or language) should weigh more heavily in grouping decisions.

### 3. Sensitive to Initialization
- K-Means results vary across runs due to random initialization.
- Even when fixing the seed, it often splits books from the same series or author into different clusters.

### 4. Poor Interpretability
- Final cluster groupings donâ€™t always make intuitive sense.
- Books that seem unrelated end up on the same "shelf," while logical groups are split.

---

## ğŸ” Planned Changes

To address these limitations, I plan to experiment with the following improvements:

### âœ… Switch to Hierarchical Clustering
- Hierarchical clustering doesnâ€™t require assumptions about cluster size or shape.
- It produces a dendrogram showing nested groupings, which can be **cut at the number of shelves** I have to form clusters.
- This approach better respects relationships in categorical data (e.g., series, authors).

### âœ… Apply Feature Weights
- Not all features are equally important.
- I plan to **weight features like author, series, and language more heavily** than general genre tags so that books from the same series or language are more likely to be grouped together.

### âœ… Improve Distance Metric
- Euclidean distance isnâ€™t ideal for high-dimensional binary vectors.
- Iâ€™ll explore **Hamming distance** or **cosine similarity**, which are more appropriate for sparse binary data.

### âœ… Optional: Use Agglomerative Clustering with Precomputed Distance
- If using a custom similarity matrix (e.g., from cosine or Hamming distances), I can feed this directly into agglomerative clustering.

---

These changes should allow shelf groupings that are more natural, interpretable, and useful â€” especially for keeping series and multilingual books together while making good use of shelf space.



| Challenge                                  | K-Means Struggles | Why                                                                 |
|--------------------------------------------|-------------------|----------------------------------------------------------------------|
| **Sparse features** (e.g. tags, genres)     | ğŸ˜¬ Yes            | K-Means relies on Euclidean distance â€” doesnâ€™t handle lots of 0s well |
| **Binary features** (tag/no tag)            | ğŸ˜¬ Yes            | Euclidean treats 0â†’1 and 1â†’0 equally, even when meaning differs      |
| **Non-spherical clusters**                 | ğŸ˜¬ Yes            | K-Means assumes clusters are round and roughly equal in size         |
| **Differing cluster sizes**                | ğŸ˜¬ Yes            | K-Means favors similar-sized clusters â€” splits large groups, merges small ones |
| **Need to keep authors/series together**   | ğŸ˜¬ Yes            | No built-in sense of hierarchy or structure                          |
| **Random cluster seeds**                   | ğŸ˜¬ Yes            | Results vary run-to-run, especially with sparse data                 |


| Feature                                          | Agglomerative Clustering |
|--------------------------------------------------|---------------------------|
| Doesnâ€™t assume shape/size of clusters?           | âœ…                         |
| Works well with sparse/binary data?              | âœ… (especially with cosine/Hamming distance) |
| Lets you preserve structure (series, authors)?   | âœ… Often groups them naturally |
| Avoids random init/seeds?                        | âœ… Deterministic process   |
| Lets you choose how many shelves later?          | âœ… Cut the dendrogram wherever you want |
